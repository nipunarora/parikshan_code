\begin{figure}[ht]
  \begin{centering}
    \includegraphics[width=0.4\textwidth]{figs/network_dup.pdf}
%    \captionsetup{justification=centering}
    \caption{Description of the Network Duplicator. In \textit{synchronized} mode: Thread 1 executes steps [1,3,5], and Thread 2 executes [2,4,6] sequentially. In \textit{asynchronous} mode: Thread 1 executes steps [1,3], Thread 2 executes [2,4], Thread 3 executes [5], and Thread 4 executes [6]}
    \label{fig:duplicator}
  \end{centering}
\end{figure}

\subsection{Proxy Network Duplicator} 
\label{sec:proxyDuplicator}

In order for \textit{live debugging} to work, both production and debug containers must receive the same input.
%This can be achieved in multiple ways, the easiest would be a port-mirroring mechanism either using software provided tap devices or in hardware switches (several vendors provide mirroring options). 
%These are both pretty common, and are blackbox and do not require much configuration.
%However, such port mirroring solution gives us minimal control on the traffic going to our test container.
A major challenge in this process is that the production and debug container may execute at different speeds (debug will be slower than production), which will result in them being out of sync.
Additionally, we need to accept responses from both servers, and drop all the traffic coming from the debug-container, while still maintaining an active connection with the client.
Hence simple port-mirroring and proxy mechanisms will not work for us.
%Hence a layer 2 level network solution is not possible as some context of the address and state are required

%Network proxies can be created at different levels in the network stack, for our purposes we have created a TCP proxy which mirrors the incoming traffic.
Our solution is a customized TCP level proxy, which also duplicates network traffic to the debug container, while maintaining the TCP session and state with the production container. 
Since it works at the TCP/IP layer, the applications are completely oblivious of it.
% and essentially works as a socket reader/writer which reads incoming TCP streams and writes these streams to two different socket connections for the production and test containers.
In this section, we discuss two strategies (we call \textit{duplication modes}) to duplicate and forward network traffic to the debug and production containers.
% we then explain a network aggregation mechanism which handles communication of the cloned debug-containers with upstream servers.  
%\input{algorithm}

\subsubsection{Duplication Modes}
\label{sec:dupMode}

%\begin{enumerate}[leftmargin=*]
%\begin{enumerate}[leftmargin=*]

%\item 
\textbf{Synchronized Packet Forwarding Mode}: 
A naive strategy is to have a single worker thread to send and receive TCP stream to the production container as well as the debug container.
%This is the simplest strategy and is quite robust as far as sending proxy data is concerned. 
To understand this better let us look at figure~\ref{fig:duplicator}: Here each incoming connection would be handled by 2 parallel threads T1, and T2 in the proxy. 
Where T1 sends data from the client to the proxy (link 1), then sends data from proxy to production (link 3), and finally from proxy to debug container (link 5). 
Whereas, T2 sends replies from production to proxy(link 4), then receives replies from debug container to proxy (link 6), which are then dropped. 
Thread T2 then forwards packets received on link 4 to the client on link 2.

By design TCP is a connection oriented protocol and is designed for stateful delivery and acknowledgment that each packet has been delivered.
Packet sending and receiving are blocking operations, and if either the sender or the receiver is faster than the other the send/receive operations are automatically blocked or throttled.

This can be viewed as follows: Let us assume that the client was sending packets at $X Mbps$ (link 1), and the production container was receiving/processing packets at $Y Mbps$ (link 3), where $Y<X$. 
Then automatically, the speed of link 1 and link 2 would be throttled to $Y Mbps$ per second, i.e the packet sending at the client would be throttled to accommodate the production server. 
Network throttling is a default TCP behavior to keep the sender and receiver synchronized.
%This behavior adheres to the default TCP protocol.
%, and if our proxy was only forwarding traffic to the production container it would be fine.
However, we also send packets to the debug-container (link 5) in the same sequential thread T1. 
Hence, if the speed of link 5 is $Z$ $Mbps$, where $Z < Y$, and $Z < X$, then the speed of link 1, and link 3 would also be throttled to $Z$ $Mbps$.

%Such a communication model effects the user-experienced delay for the targeted SOA application, and is against the guiding principal of \parikshan.
The speed of the debug container is likely to be less than the production container, hence this would definitely impact the performance of the production container.
Clearly, this is a non-optimal solution. 
Next we discuss an asynchronous packet forwarding scheme, which avoids any slowdown to the production container.

%It especially works well with standard SOA architectures with small incoming packet flows, 
%as it means that the delay caused in the test and production container would be minimal.
%However, a major disadvantage of this approach is that every connection received would 
%finish a round trip connection with production container before sending the packets to the test-container.
%This would obviously delay all communication to the test-container by the amount of time it takes for the production container to respond. 
%However, more importantly it also effects the speed of the production container for every subsequent flow, 
%since it will have to wait for the time taken to send the packets to the test container.
%We assume a worker thread model for our proxy, where each connection is managed separately by one worker thread, 
%since our target is a service oriented architecture there should be frequent small connections from multiple users, hence a threading model is important.

%\item 
\textbf{Asynchronous Packet Forwarding Model}: 
%In the synchronized mode the debug container, can impact the speed of the production container as well. 
%The main reason for this is because of blocking sends being used to forward packets from the client to the debug-container and production container by the same sequential process.
In the asynchronous packet forwarding mode (see figure \ref{fig:duplicator}): we use 4 threads T1, T2, T3, T4 to manage each incoming connection to the proxy.
Thread T1 forwards packets from client to proxy (link 1), and from proxy to production container (link 2). 
It then uses a non-blocking send to forward packets to an internal pipe buffer shared between thread T1, and thread T3. 
Thread T3, then reads from this piped buffer and sends traffic forward to the debug-container. 
Similarly Thread T2, receives packets from production container, and forwards them to the client, while Thread T4, receives packets from the debug-container and drops them.

The advantage of this strategy is that any slowdown in the debug-container will not impact the production container's communication as a non-blocking send is used to forward traffic to the production container. 
A side-effect of this strategy is that if the speed of the debug-container is too slow compared to the production container, it may eventually lead to a buffer overflow. 
We call the time taken by a connection before which the buffer overflows it's \emph{debug-window}.
We discuss the implications of the \emph{debug window} in section \ref{sec:window}.  
%In this strategy we have two worker threads which simultaneously handle sending and receiving tcp streams for the production container as well as the test container.
%Whenever any event happens the connection is established, and the incoming message is read to a buffer, this buffer is then sent to a worker thread for forwarding the data to the production container, and another thread for forwarding the data to the test container. 
%This means that unlike our previous model, the processing time of either the production container or the test container will not effect the performance of the other.
%We have a slightly modified worker thread model, in which each incoming connection is managed by two worker threads one for communication with the production and the other for the test.

%\item \textbf{Asynchronous Load Balanced Packet Forwarding Model}

%It is still possible that there will be slowdown, and a short packet window because of the overhead of running test-cases in the test container. 
%This would mean that the test container will have a short time-window to execute test-cases.


%\end{enumerate}

%The algorithm for each of the packet forwarding modes has been described in Algorithm.\ref{algoDuplication}. 
%As an added explanation, the only difference between communication with the production container v/s the test container is that the when the proxy receives the message back from the production container, it sends the stream back to the client, on the other hand for the test container the packets are simply received and then dropped.


\iffalse

Since the speed of input from the client is not in sync production-container, we need to buffer incoming traffic and relay it to the the TCP Connector(Node 6, figure \ref{fig:duplicator}) as soon as the test-container is ready for new traffic. 
Since parallel connections can be initiated by the client on the same TCP port, the Buffer-Manager creates multiple buffers for each connection, and initiates new connections by following the same causal flow of the packets received from the client. 
Unlike record-replay systems \parikshan does not claim to have strong consistency requirements, and does not need to exactly follow the production container as the goal is not reproduce all non-determinism in the production environment, but instead capture input non-determinism, and complete system state from a given point of time.
We discuss consistency requirements further in section \ref{sec:consistency}

\fi
