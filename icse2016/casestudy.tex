\section{Case Studies}
\label{sec:casestudy}

\begin{table*}[ht!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\begin{tabular}[c]{@{}c@{}}\textbf{Bug} \\ \textbf{Type}\end{tabular}                        & \textbf{Bug Desc}                                                       & \textbf{Application}           & \begin{tabular}[c]{@{}c@{}}\textbf{Tool} \\ \textbf{Used}\end{tabular}       & \begin{tabular}[c]{@{}c@{}}\textbf{Debug} \\ \textbf{Mechanism}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Slow-}\\ \textbf{down}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Nodes}\\ \textbf{Cloned}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Comments/}\\ \textbf{Bug Caught}\end{tabular} \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Performance}\\ \textbf{Bug}\end{tabular}} & \#15811                                                        & MySQL                 & iProbe                                                     & Exec. Trace                                                & 1.5x                                                 & 1                                                      & Yes                                                            \\ \cline{2-8} 
                                                                           & \#45464                                                        & Apache                & iProbe                                                     & Exec. Trace                                                & 1.4x                                                 & 1                                                      & Yes                                                            \\ \cline{2-8} 
                                                                           & \#49491                                                        & MySQL                 & iProbe                                                     & Exec. Trace                                                & 1.8x                                                 & 1                                                      & Yes                                                            \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Memory} \\ \textbf{Leak}\end{tabular}}    & Injected                                                       & Glassfish             & \begin{tabular}[c]{@{}c@{}}VisualVM/\\ mTrace\end{tabular} & \begin{tabular}[c]{@{}c@{}}Memory\\ Profiling\end{tabular} & 3x                                                   & 1                                                      & Yes                                                            \\ \cline{2-8} 
                                                                           & Injected                                                       & MySQL                 & Valgrind                                                   & memcheck                                                   & N.A                                                   & 1                                                      & \begin{tabular}[c]{@{}c@{}}Yes, with\\ overflows\end{tabular}  \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Config.} \\ \textbf{Errors}\end{tabular}} & DNS                                                            & JBOSS                 & DTrace                                                     & Tracing                                                    & 1.3-4x                                               & 2                                                      & Yes                                                            \\ \cline{2-8} 
                                                                           & \begin{tabular}[c]{@{}c@{}}Max no. \\ of threads\end{tabular}  & MySQL                 & DTrace                                                     & Tracing                                                    & 1.5x                                                 & 2                                                      & Yes                                                            \\ \hline
%\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Concurren \\ Bug\end{tabular}}  & \begin{tabular}[c]{@{}c@{}}Atomicity \\ Violation\end{tabular} & Apache                &                                                            &                                                            &                                                      &                                                        & Maybe                                                          \\ \cline{2-8} 
%                                                                           & \begin{tabular}[c]{@{}c@{}}Race\\ Condition\end{tabular}       & Apache                &                                                            &                                                            &                                                      &                                                        & Maybe                                                          \\ \hline
%\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Crash \\ Bugs\end{tabular}}     & Server Crash                                                   & Apache                & N.A                                                        & N.A                                                        &                                                      &                                                        & Cannot Debug                                                   \\ \cline{2-8} 
%                                                                           & Request Crash                                                  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}                                      & \multicolumn{1}{l|}{}                                      & \multicolumn{1}{l|}{}                                & \multicolumn{1}{l|}{}                                  & Yes                                                            \\ \hline
\end{tabular}\caption{Case-Study of production system bugs, with approximate slowdowns, debug mechanisms, and tools used.}
\label{table:casestudy}
\end{table*}

%Here we show how \parikshan can be applied to some real-life bugs.
%Table \ref{table:casestudy} shows several bugs which we debugged using \parikshan, and the various techniques we used to localize the bug.
%Parikshan enables the users to freely run any test-case in the test-container while not effecting the production container. 
%At the same time the output of these tests should not effect the functionality or the performance of the production system.
%The main advantage of such a system can be seen in service oriented applications which are user facing and can hence ill-afford to be shutdown for inspecting bugs.
%As mentioned earlier, another major advantage is that we are able to capture live user-input. \\

%\subsection{Debug Tools and Mechanisms}
%\label{sec:debugMechanisms}

Production applications are usually deployed with transaction logs, and error logs that alert the operator of any anomalous behavior.
%Often the information in such logs is insufficient to find the root-cause, and can give false-positives or skip errors altogether.
Debuggers use these logs, to run offline test-cases in order to find the root-cause of any bugs. 
This process may need multiple iterations between the operator and developer, as more information may be required from the production system. This eventually increases the time to debug the error.
%Since only minimal dynamic instrumentation is allowed, developers go through a trial and error process of increasing the coverage of instrumentation 
%(or have higher instrumentation with the production service suffering an overhead).

%In this section we briefly mention various debug mechanisms, and what they generally involve. 
%We discuss how they can be successfully applied to bug cases we found in production systems in the next subsection.
The idea behind \parikshan is to allow operators to use debugging tools and mechanisms in the production environment, that one would normally use in the development environment.
These tools allow developers to better understand program flow and data flow, which is key to localizing the root cause of any bug.
Dynamic instrumentation tools like DTrace~\cite{dtrace}, and iProbe~\cite{iProbe} can generate \emph{execution traces}, which give insight into the program flow (functions, and conditional statements executed).
iProbe has statically compiled ``hooks'' in the beginning and end of functions in the target applications, which can be patched with dynamic instrumentation on-the-fly.
On the other hand, DTrace uses interrupt mechanisms to insert a probe at any point in the application using symbolic information. 
%Data flow on the other hand requires much finer instrumentation. 
Similarly, tools like dyninst~\cite{dyninst} allow users to track data-flow of variables in the code.
\emph{Memory profiling} tools like Valgrind~\cite{valgrind}, PIN~\cite{pin}, and VisualVM~\cite{visualvm} give insight into memory allocation and deallocation, thus helping in catching memory leaks.
%Another tool, Valgrind~\cite{valgrind} can do systematic checking of memory allocation, but has a high overhead and requires application restart.
%On the other hand, profiling tools like VisualVM~\cite{visualvm}, and PIN~\cite{pin} can be dynamically attached to track memory usage. 
%The downside to this is that it halts progress in the application, unlike execution trace which imposes a slowdown. 
%Other mechanisms such as \textbf{delta-debugging}, and \textbf{heap-dump} allow users to modify variables to check if that resolves problems hence changing it's behavior.
Next, we discuss how these techniques can be applied to different categories of real-life bugs with the help of the \parikshan framework:

%\subsection{Bug Categories}
%\label{sec:bugCategories}

%\subsubsection{Performance Bugs}
%\label{sec:performanceBugs}
\noindent
\textbf{Performance Bugs}: One of the most subtle problems that manifest in production systems is performance bugs.
% especially in usecases which were earlier not thought of during stress testing.
These bugs do not usually lead to crashes, but cause significant impact to user satisfaction.
%The problem can manifest itself because of some configuration parameter, or redundant function calls, non-optimal path, and can require either a bug-fix, or a new feature addition.
A casestudy~\cite{shanluPerf} showed that a large percentage of real-world performance bugs can be attributed to uncoordinated functions, executing functions that can be skipped, and inefficient synchronization among threads (for example locks held for too long etc.).
Typically, such bugs can be caught by function level execution tracing and tracking the time taken in each execution function.
Another key insight provided in~\cite{shanluPerf} was that two-thirds of the bugs manifested themselves when special input conditions were met, or execution was done at scale. 
Hence, it is difficult to capture these bugs with traditional offline white-box testing mechanisms.

%\noindent
In an empirical evaluation of three such real-world bugs, we used \parikshan and iProbe~\cite{iProbe} (a dynamic instrumentation tool) to capture a representative function trace.
The tracing caused a 1.5x - 2x slowdown in the targeted session of the application,  with no impact on the production system.
For one of the bugs in  MySQL (Bug 15811), it was reported that some of the user requests which were dealing with complex scripts (Chinese, Japanese), were running significantly slower than others.
To evaluate \parikshan, we re-created a two-tier client-server setup with the server (container) running a buggy MySQL server. 
We generated a random workload of user-requests, including queries that triggered the bug.
Next, we created a live clone of the MySQL container.
We then inserted function level instrumentation in MySQL for higher level functions, object constructors, and destructors belonging to each module in the code.
Finally, we used the query handler to trigger instrumentation for queries containing complex scripts.
%For all transactions with insert queries with a complex character set, we triggered instrumentation to track execution in high granularity functions in insert and character\/string module.
We then compared the profile (time taken in each function) of the queries for complex scripts, with the queries for Latin scripts.
Using trial-and-error, and iteratively digging into deeper functions, we were able to localize the bug to string compare functions in MySQL.
%Using the profile for regular latin charactersets, and comparing it to complex charactersets (chinese, japenese) we were easily able to localize the bug to the compare functions in the string module.
%Using a trial and error method we profiled high granularity functions in MySQL and gradually looked at finer granularity modules to isolate the performance problem.
%This allowed us to successfully isolate the function with the performance problem.
%The MySQL queries, were only a few KB's so we were able to queue up several requests in the pipe and there was no divergence in the output produced by both the debug container and the production container. 


%\subsubsection{Memory Leaks}
%\label{sec:memoryLeaks}
\noindent
\textbf{Memory Leaks}: %Localizing a memory leak takes us further than simply execution tracing. 
Memory leaks are a common error in service oriented systems, especially in C/C++ based applications which allows low-level memory management by users.
These leaks build up over time, and can cause slowdowns because of resource shortage, or crash the system.
Debugging leaks can be done either using systematic debugging tools like Valgrind, which use shadow memory to track all objects, or memory profiling tools like VisualVM, mTrace, or PIN, which track allocations, de-allocations, and heap size.
Although valgrind is more complete, it has a very high overhead, and needs to capture the execution from the beginning to the end (i.e., needs application restart).
On the other hand, profiling tools are much lighter, and can be dynamically patched to a running process.

For our empirical evaluation, we injected repetitive memory allocations to an object in a glassfish app-server, and sent requests to app-server to trigger these allocations.
We then, used \parikshan to clone \& create a debug-container, where we used VisualVM to track the increase of heap-size, and object allocations.
Tracking memory allocation allowed us to localize the class file causing the memory leak, which greatly simplifies finding the root-cause.
We experienced a slowdown of 3x, but did not observe any overflow in the buffer.

We also used memory leak detection using Valgrind in MySQL for an injected memory leak in the request handler for MySQL.
We wish to stress here, that while tools like valgrind, need application restart, and will most likely cause extremely high slow-downs in the debug-container, \parikshan can still be helpful, as it can debug a small part of the target system in isolation, thereby quickly resolving errors.
Application service restart may not lose context in all applications, especially a database application has a persistent database, and may exhibit errors even after the restart.
With Valgrind, we saw requests being dropped in the proxy because of the high overhead, but we were able to capture and trace memory leaks for several requests.


\iffalse
Memory leak analysis can be done by doing memory allocation function profiling (malloc, free etc.) or using tools like Valgrind~\cite{valgrind}, which use shadow memory techniques to trace all actively allocated objects.
While shadow-memory tools are good for development debugging, it cannot be applied to real-production environments as they cannot be dynamically attached to a running service, and the process needs to be closed to compute which objects remained allocated or to show dangling reference (actively allocated memory may or may not be used in the future) etc.. Additionally these tools are resource hungry and have a high overhead (~4-6X).

In our memory leak case study, we first profiled the application to find transactions that could potentially be the cause of the memory leak. 
We then filter out everything except the suspect transactions, and restart mysqld service with valgrind memcheck tool turned on.
The valgrind tool was able to summarize and point to the root-cause of the bug which we validated from the bug report.
In this case the considerable overhead because of valgrind, and the fact that it serialized the execution (other transactions were not able to follow), led to transactions being dropped in the proxy. 
However, this did not impact us in finding the root cause.

%\noindent
In another memory leak scenario for Java Glassfish~\cite{glassfish} app server, we were able to apply a memory profiler (visualVM), to look at allocation and deallocation patterns.
This has comparatively lesser overhead, and can point to stale objects, or can profile how different classes are using the heap memory.
%In cases where memory leak exhibits because of the application state after a long running execution, memory profiling may be a better choice than shadow memory tools, as they require a service restart.
Using memory profiling in \parikshan will allow users to capture the ``long-running'' context of the application which may be important to capture the bug.
\fi

\noindent
\textbf{Configuration Errors}:
Configuration errors are usually caused by wrongly configured configuration parameters, i.e., they are not a bugs in the application, but bugs in the input (configuration).
Configuration bugs usually get triggered at scale, or for certain edge cases, making them extremely difficult to catch.

For our empirical evaluation, we created a 3 tier (Apache, JBoss, MySQL) service using Java PetStore~\cite{petstore} a J2EE example application.
We inserted a configuration error by setting an arbitrarily low number for the max. number of requests MySQL can handle.
We then triggered requests to PetStore, and found some of them timing out.
Using \parikshan, we cloned both JBoss and MySQL servers, to captured the flow of queries across distributed machines.
We traced higher object constructors, and library calls to track the execution.
This allowed us to see that the MySQL server was not creating worker threads for the incoming requests(no calls to libc fork events).
Debug logs had been turned off for optimization, hence the error could not be easily reported. 
\parikshan was able to successfully localize the error to mis-configuration in the ``maximum no. of worker thread'' allowed parameter.
While the error may seem simple, without \parikshan an offline testbed would be required with a large enough test workload.


% it was reported that certain queries were returning empty or ``null'' pages. 
%The operator can potentially find out which machines in the cluster were affected by this bug.
%Since debug logs have not been turned on we can only see the path the query is taking but do not see any error report generated.
%This can be used by the operator, to find which containers need to be cloned. 

%\subsubsection{Configuration Bugs}
%\label{sec:configurationBugs}

%\subsubsection{Concurrency Bugs}
%\label{sec:concurrencyBugs}

%\textbf{Concurrency Bugs}: Concurrency bugs are caused because of non-deterministic execution in multi-threaded applications.
%These bugs are particularly difficult to catch in white-box debugging as 

%\subsubsection{Integration Bugs}
%\label{sec:integrationBugs}
%\subsection{Debugging Techniques}
%\label{sec:debugTechniques}
%\par \noindent
%\subsection {CaseStudy 2: Performance Profiling, using dyninst}
%Demand Driven Testing - https://www.cs.virginia.edu/~soffa/Soffa_Pubs_all/Conferences/Demand-driven.Misurda.2005.pdf
%@Nipun- I don't think either of the next two make a lot of sense
%\par \noindent \textbf{CaseStudy 3: A/B Testing}
%A/B Testing 
%http://www.polteq.com/wp-content/uploads/2013/02/testingexperience20_12_12_Marc_van_t_Veer.pdf
%\par \noindent \textbf{CaseStudy 4: Fault Tolerance Testing}
%Fault Injection to look at fault tolerance
%Fault Injection in netflix http://techblog.netflix.com/2012/07/chaos-monkey-released-into-wild.html
%Security Honeypots? 
\iffalse
%\par \noindent
\subsection{Case-Study 1: Debugging using Execution Tracing of MySQL bug 18511}  
Performance profiling such as function execution trace, execution time, resource usage etc., is often used to indicate and localize performance bugs in real world systems. 
While performance profiling is simple to implement, it obviously incurs an overhead and will effect user-experience of the target system.
Effectively, this means that despite it's advantages, the amount of profiling that can be done in a production system is extremely limited. 

As our first case study we focused on profiling and capturing a performance bug in a session with several randomly created user transactions to MySQL. 
It was reported by users, that some of the user requests were running significantly slower than others.
To test out \parikshan to catch this bug, we re-created a 2 tier client-server setup with the server(container) running a buggy mysql server application, and made a random workload with several repetitive instances of queries triggering the bug.
We initiated debugging by creating a live clone of the container running the mysql server.
Using a trial and error method we profiled high granularity functions in mysql and gradually looked at finer granularity modules to isolate the performance problem.
This allowed us to successfully isolate the function with the performance problem.

We believe this case study shows a classical performance bug, where \parikshan can be used.
Firstly, it's a performance bug which is non-critical i.e. does not lead to crashes etc.
\parikshan has been designed to look into real live bug diagnosis, rather than looking at bugs after a crash etc.
\footnote{A lot of SOA applications are fault tolerant, and can continue even after the crash by relaunching processes etc. Potentially \parikshan can also be used in such a scenario to trace the crash itself.}
Secondly, we assume that the user-input (which caused the bug), occurs fairly frequently.
In this case, a database query which looks into data containing chinese characters, or japanese characters could be a fairly common occurence.
In our experience we found several such performance bugs which occur in only a small percentage of user transactions.
These bugs are often difficult to catch as they happen only in corner case inputs, and generally do not lead to application crashes.
\parikshan would be useful to debug such performance errors, by giving deep insight in a live running system.
\fi

