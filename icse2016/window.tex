
\subsection{Debug Window}
\label{sec:window}

%At the time of the live cloning, the testing container has an identical status and receives the same input as the production container. 
%Hence, any test-cases run in the testing container, should faithfully represent the status of the production container.
%However, an obvious down-side to any debugging/testing is that it will add an overhead on the performance of the test-container as compared to the production environment.
%This would mean that the execution of requests in the test-container will lag behind the production container. 
%To avoid this slowdown impacting the actual production container, 
%\noindent
In the asynchronous forwarding mode, an unblocking send forwards traffic to a separate thread which manages communication to the debug-container. 
This thread has an internal buffer, where all incoming requests are queued, and subsequently forwarded to the debug-container. 
The incoming request rate to the buffer is dependent on how fast the production container manages the requests (i.e. the production container is the rate-limiter).
Whereas the outgoing rate from the buffer is dependent on how fast the debug-container processes the requests.
Because of instrumentation overhead in the debug-container, the outgoing rate is likely to be less than the production container.
The time period till buffer overflow happens is called the \emph{debug-window}.
This depends on the size of the buffer, the incoming request rate, and the overhead induced in the debug-container. 
For the duration of the debugging-window, we assume that the debug-container faithfully represents the production container. 
Once the buffer has overflown, the production container may need to be cloned again to ensure it has the same state.

%\noindent
The debug window size also depends on the application behavior, in particular how it launches TCP connections. 
\parikshan generates a pipe for each TCP connect call, and the number of pipes are limited to the maximum number of connections allowed in the application.
Hence, buffer overflows happens only if the requests being sent in the same connection overflow the queue.
For webservers, and application servers, the debugging window size is generally not a problem, as each request is a new ``connection'', hence the proxy is able to tolerate significant slowdowns.
Database servers, and other session based services usually have small request sizes, but multiple requests can be sent in one session which is initiated by a user. 
In such cases, the amount of calls in a single session can eventually have a cumulative effect to cause overflows.
Photosharing, file sharing and other datasharing services transfer a lot of data in a single burst over each TCP connection. 
These protocols will have an overflow relatively sooner, and will be able to tolerate only small amounts of slowdown. 

To further increase the \emph{debug window}, we propose load balancing debugging across multiple debug-containers, which can each get a duplicate copy of the incoming data. 
This would mean that there are multiple threads handling the incoming connection, one for the production container, and one for each of the debug containers.
We believe that such a strategy would significantly reduce the chance of a buffer overflow in cases where the debug-container is significantly slower.
We evaluate debug-window overflows in section~\ref{sec:timewindowPerformance}.

%\iffalse
%In this section we try to model the testing window by using concepts well used in queuing theory(for the sake of brevity we will avoid going into too much detail, readers can find more about queuing theory models in~\cite{queueWiki}).
%The buffer overflow of our test-container can be modeled as a M/M/1/K queue (Kendall's notation~\cite{kendall}), for our simplest asynchronous model, and as a M/M/c/K queue in the asynchronous load-balanced model.
%An M/M/1/K queue, denotes a queue where requests arrive according to a poisson process with rate $\lambda$, that is the inter-arrival times are independent, exponentially distributed random variables with parameter $\lambda$ . 
%The service times are also assumed to be independent and exponentially distributed with parameter $\mu$. Furthermore, all the involved random variables are supposed to be independent of each other. 
%Further, the notation specifies that there are $c$ queues/ or alternatively $c$ servers managing the requests, and the queues is of a finite capacity, i.e. the queue can accommodate a maximum of K requests.
%In our case $\lambda$ denotes the rate at which requests arrive to the buffer from the production container, and $\mu$ denotes the processing time overhead of each request in the test-container (to simplify the problem, we ignore the actual processing time of the test-container, as the incoming rate $\lambda$ is already synchronized with the production container processing time, and it is only the overhead added by the test-container which matters). 
%In our simple asynchronous forwarding strategy, we have $c=1$ as we have only a single test-container, potentially as explained earlier, in a load-balanced asynchronous model this could be extended to $c$ servers to increase the time window.

%Now based on this notation the expected size of the time window is :
%\texttt{Nipun's note - This equation and is tied to the evaluation in section 7.2, which is being worked on}
%\fi


%In queueing theory terms, the size of the testing window can be modelled as a special case of continuous-time Markov process also called the \emph{birth-death} process. 
%For the sake of brevity we will only briefly explain the basics of queueing theory
%It is possible that in heavy load conditions the buffers in the Buffer Manager may overflow. 
%Depending on the use-case the BufferManager can then trigger the CloneManager, which closes the analysis time-window and launches a fresh clone.
%Incoming traffic may potentially have a cumulative effect which will eventually lead to a buffer overflow in the BufferManager, hence leading to packet drops, and eventually leading to a modified state in the testing container, which can no longer faithfully represent the production container.
%This behavior is similar to packet dropping in classical TCP buffers, which rely on re-transmission of incoming packets from the client.
%However in our case if packets are dropped the client cannot re-transmit them as it not in sync with the test container.
%Hence in some cases \parikshan analysis, has to be restricted to a time-window dependent on the load of incoming traffic and the slowdown experienced by the test-container.

%Let the rate of incoming traffic be \lambda
