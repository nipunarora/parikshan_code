\section{Problem Formulation}
\label{sec:model}

As explained in section~\ref{sec:design}, the asynchronous packet forwarding in our network duplication results in a ``debug window''. 
The ``debug window'' is the time before the buffer of the debug-container overflows because of the input from the user. 
The TCP connection from end-users to production-containers are synchronized by default.
This means that that the rate of incoming packets is limited by the amount of packets that can be processed by the production container. 
On the other hand, packets are forwarded asynchronously to an internal-buffer in the debug-container.
The duration of the ``debug window'' is dependent on the incoming workload, the size of the buffer, and the overhead/slowdown caused due to instrumentation in the debug-container.

In this section we try to model the testing window by using concepts well used in queuing theory(for the sake of brevity we will avoid going into too much detail, readers can find more about queuing theory models in~\cite{queueWiki}).
The buffer overflow of our test-container can be modeled as a M/M/1/K queue (Kendall's notation~\cite{kendall}), for our simplest asynchronous model, and as a M/M/c/K queue in the asynchronous load-balanced model.
An M/M/1/K queue, denotes a queue where requests arrive according to a poisson process with rate $\lambda$, that is the inter-arrival times are independent, exponentially distributed random variables with parameter $\lambda$ . 
The service times are also assumed to be independent and exponentially distributed with parameter $\mu$. Furthermore, all the involved random variables are supposed to be independent of each other. 
Further, the notation specifies that there are $c$ queues/ or alternatively $c$ servers managing the requests, and the queues is of a finite capacity, i.e. the queue can accommodate a maximum of K requests.
In our case $\lambda$ denotes the rate at which requests arrive to the buffer from the production container, and $\mu$ denotes the processing time overhead of each request in the test-container (to simplify the problem, we ignore the actual processing time of the test-container, as the incoming rate $\lambda$ is already synchronized with the production container processing time, and it is only the overhead added by the test-container which matters). 
In our simple asynchronous forwarding strategy, we have $c=1$ as we have only a single test-container, potentially as explained earlier, in a load-balanced asynchronous model this could be extended to $c$ servers to increase the time window.

Now based on this notation the expected size of the time window is :
