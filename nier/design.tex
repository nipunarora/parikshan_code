\section{Approach}
\label{sec:design}

%In this section we begin with an explanation of live cloning to explain to the reader some the key challenges in designing a sandbox. 
%We then give a system overview of \texttt{Parikshan}, explaining each of its components. 
%Next, we explain how it inserts test cases, into the test harness, and finally we explain 
%how a user can use the \texttt{Parikshan} api to insert test cases in the test harness.

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figs/workflow2.eps}
    \caption{Backend wrapped around with \parikshan run-time}
    \label{fig:workflow}
  \end{center}
\end{figure}

%\subsection{Workflow and Architecture}
%\label{sec:workflowArch}

%The basic workflow of \parikshan is as described in Figure \ref{fig:workflow}. 
%\parikshan can be applied at the boundary of every tier of a multi-tier system. 
%In figure \ref{fig:workflow} we show parikshan being applied to a single tier boundary 
%(communication from a webserver to a backend server) to showcase a simple example, 
%this process can be easily scaled depending on the complexity of the system design. 
%The architecture is driven by a proxy request duplication agent( see section \ref{ and a live clone manager.

\subsection{System Model}
\label{sec:systemOverview}

%Each instance of \parikshan can target only one tier at a time.
%However, multiple instances can be orchestrated together when cross tier results need to be correlated.
%Figure \ref{fig:workflow}, shows a simple client-database server example, where our system is applied to the database(backend).
The two main components of \parikshan are (see Fig \ref{fig:workflow}): the clone manager (for live cloning), and a custom proxy (clone network requests).
%As explained earlier basic workflow of our system is to duplicate all network requests to the production backend server and a ``live cloned'' test container.
%Traffic duplication is managed by our proxy network duplicator (see section \ref{sec:proxyDuplicator}), which uses several different strategies to clone user input to our test-container, with minimal impact on the production container.
%Another core aspect of our design is ``live cloning''; this is the process by which a production container (in this case our backend service), can be cloned to create a test-container which has the same file system and process state. 
%Cloning and syncing between the production container and the test-container is managed by our clone manager, which uses user-space containers OpenVZ\cite{openvz} and a variant of live migration to implement the cloning.
%Next, we explain each of the modules in detail.
%The architecture can be divided into two parts: (1) A Proxy Network Duplicator, (2) container clone manager

%\begin{itemize} 

\subsection{Clone Manager} 
\label{sec:CloneManager}

The clone manager triggers scripts to do a ``live clone'' of the target container, and tells our custom proxy to redirect the traffic to both the debug and production containters. 
Live cloning copies the target container, and results in two containers (original, and a new one), with the exact same system state.
The cloning operation is hidden from the end-user as there is no loss of service of the application, and minimal suspend time to do the actual cloning.
%\subsubsection{How does cloning work?}
%\label{sec:cloning}

%While the focus of our work is not to support container migration, or to make changes to the hypervisor, we need to tweak the way typical hypervisors offer live migration for our purposes.
%Before moving further we wish to clarify that instead of the standard live migration supported by hypervisors, \parikshan requires a cloning functionality. 
\textbf{Challenges }
In contrast with live migration, where a container is copied to a target destination, and then the original container is destroyed, the cloning process requires both containers to be actively running, and be still attached to the original network.
Hence, there are two things that cloning needs to address: (1) It needs to copy the container files with minimal suspend time, and (2) it needs to maintain identical network identities in the network.
%The more challenging aspect is that there are two containers with the same identities in the network. 
This is important, as the operating system and the application processes running in it may be configured with the IP address, or other networking operations, which cannot be changed without leading to a crash of the system.
Hence the same network identifier should map to two separate addresses.
On the other hand, the same IP address and MAC address cannot be kept for both the production and debug-container as that would lead to conflict. 


\textbf{Basic Workflow }
%This cloning requires some tweaking, and modification in both how compute migration is handled, and especially how the network migration is handled. 
%Live migration refers to the process of moving a running virtual machine, guest os or container from one host node(physical machine) to another, without disconnecting any client or process running within the machine. 
Live migration is supported by most well known Hypervisors (such as vmware, virtualbox, xen, qemu, kvm) with different trade-offs.
However, live migration is not exactly live, although it may look like that to the user. 
There is always a short time period, usually of a few seconds, during which the target container is suspended.
Different approaches have been proposed to reduce this suspend time. 
Our approach is as follows:
(1) A container with the exact same configuration is created in the physical server where the debug-container will be run.
(2) A copy process (\textit{rsync}) is initiated to match the file system of the production and debug container. This phase is called the pre-copy stage where the underlying hypervisor takes a snapshot, and copies all the container pages from the source to the destination. 
(3) Once the pre-copy phase is finished, the VM is temporarily suspended, and all the ``delta pages'' are transferred to the target node. Delta pages here refers to pages which have changed from the time of the first snapshot (pre-copy phase).
Once the second rsync finishes both the containers are in the same state. 
The first rsync takes care of most of the system storage, by doing the copy in the background, and does not need to suspend the container. The second rsync moves only modified pages, hence needs a short suspend time.
(4) Finally the proxy creates a duplicate connection, and both containers are restarted from the suspend state.

%This, two rsync process reduce the suspend time as the first one moves most of the data, while the container is still up and running, and the second one moves only the changes made during the time period .
%\footnote{Network migration is managed by the IAAS which publishes the same MAC address for the copied VM. 
%Since the identity of the target container remains the same, the IAAS is able to give it same IP Address, and network traffic is rerouted after the identity is published in the network}.
%To reduce the down-time memory pages are transferred probabiliticly based on priority, and incase a page is not found a network fault is generated which executes a fetch from the original VM.
%There are two ways to resolve this : 
While the actual packet forwarding to the containers is taken care of by the proxy, to resolve network mapping of the same identifier to two different containers, we use the following mechanisms: 
(1) Internal Mode: Host each container behind their own network namespaces, on the same host machine, and configure packet forwarding to both containers such that the proxy can communicate to them. 
Network namespaces are used by several hosting providers, to launch VM's with the same private IP Address, in a shared network domain \cite{OpenStack}. 
(2) External Mode: host both containers in different machines with port forwarding setup to forward incoming TCP requests to containers behind a NAT. 
This is slightly more scalable and provides separation of network and compute resources. 
%However, an obvious downside to this approach is that it needs a new VM to be allocated.
%This leads us to two different modes of implementation, which we discuss in the next section.

%As of now, we have implemented the second approach for our proof of concept mostly because of the ease of implementation.
%Further a packet level sniffer or mirror port would keep the same buffer and potentially timeout.
%To allow for some load-balancing and an application aware buffer, we went for a web application level proxy server to duplicate traffic to both containers. 
%Further in this section we explain how we deal with these challenges.
\iffalse
\subsubsection{Basic Design \& Modes}

The Clone Manager itself is just an interface which interacts with an \textit{agent} installed in each container host.
The clone manager dictates the frequency of cloning/syncing operations, as well as  coordinates setup operations/orchestration etc.
Communication between clone manager, and the agent is done using RPC calls implemented using apache thrift \cite{thrift}.
The agent itself is the driver for live cloning, and performs rsync operations, snapshot, transferring and starting the image.

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figs/modesCloning.eps}
    \caption{External and Internal Mode for Live Cloning: P1 is the production container, and T1 is the test container, the Clone Manager interacts with an Agent which has drivers to implement live Cloning}
    \label{fig:modesCloning}
  \end{center}
\end{figure}


Test and production containers can be allocated in various schemes: we call these schemes \textit{modes}. 
Broadly speaking the clone manager works in 2 modes (see figure.\ref{fig:modesCloning}) : 
\begin{itemize}

\item \textit{Internal Mode}: In this mode we allocate the test-container and  production containers to the same host node. 
This would mean less suspend time, as the production container can be locally cloned ( instead of streaming over the network). 
Requires the same amount of resources as the original production container (number of host nodes remain the same), hence could potentially be cost-effective.
On the down-side, co-hosting the test and production containers, could have an adverse effect on the performance of the production container, hence effecting the user.
As we can see in figure.\ref{fig:modesCloning} the production container P1 and test container T1 are both hosted within the same physical host, and with the same ip address, but their network is encapsulated within different network namespaces to sandbox them.
The duplicator is then able to communicate to both these containers with no networking conflict.

\item \textit{External Mode}: In this mode we provision an extra server as the host of our test-container (this server can host more than one test-containers), 
While this mechanism can have a higher overhead in terms of suspend time (~3 seconds, dependent on process state), and it will require provisioning an extra host-node, the advantage of this mechanism is that once cloned, the VM is totally separate and will not effect the performance of the production-container.
We believe that such a mode will be more beneficial in comparison to the internal mode, as cloning is likely to be transient, and it is often more important to not effect user experience, which in the internal-mode can be effected as the test, and production run in the same host (resource contention etc.).
As we can see in figure.\ref{fig:modesCloning} the production container P1 and test container T1, are hosted on two different host machines, and are encapsulated behind a NAT\cite{nat} (network address translator), hence they each have their IP's in an internal network thereby avoiding any conflict.\footnote{ Another additional mode can be \textit{Scaled Mode}: This can be viewed as a variant of the External Mode, where we can scale out test cases to several testing containers which can be used to distribute the instrumentation load to reduce the overhead. This is currently out of the scope of this paper, however we aim to show this in a future publication.}

\end{itemize}

%\RestyleAlgo{boxruled}
%\LinesNumbered

%\\ \\
%\noindent \textbf{Implementation}\\

\subsubsection{Algorithm}
\fi
%The Clone Manager is responsible for creating a live running ``clone'' of the production container and launch it as the test-container. 
%In our current setup cloning is done for each target production environment in the same physical host machine 
%(we can clone to a different physical host as well, however for optimization purposes we have assumed that they will always be in the same local network).

%In our current implementation, we are using OpenVZ\cite{openvz} as our container engine, and have modified the migration mechanism in vzctl \cite{vzctl} to make it work for live cloning instead. 
%We tested this out on multiple VM's acting as host nodes for OpenVZ containers. 
%To make the cloning easier and faster, we used OpenVZ's \textit{ploop} devices \cite{ploop} to host the containers. 
%\textit{Ploop} devices are a variant of disk loopback devices where the entire file system of the container is stored as a single file. 
%This makes features such as syncing, moving, snapshots, backups and easy separation of inodes of each container file system.

\iffalse
The algorithm for live cloning is explained below: 

\begin{algorithm}[ht]
\begin{algorithmic}
   \caption{Algorithm for Live Cloning using OpenVZ 
   \label{algCloning}}
 \begin{enumerate}
   \item Safety Checks ( ssh without entering a password, and version checking) 
   \item Runs rsync of container file system (\textit{ploop} device) to the destination server  
   \item Checkpoints and suspend the container 
   \item Runs a second rsync of the \textit{ploop} device to the destination  
   \item Start container locally 
   \item Set up port forwarding and packet duplication
   \item Starts the container on the destination 
  \end{enumerate}
\end{algorithmic}
\end{algorithm}

Let us imaging we are cloning production container C1 on Host H1 as test container C2 on Host H2. 
The initial setup requires certain safety checks and pre-setup to ensure easy cloning, these include: ssh-copy-id operation for accessing without password, checking pre-existing container ID's, check version of vzctl etc. 
These ensure that H1 and H2 are compatible, and ready for live-cloning.
Next, we run an initial rsync of container C1, from Host H1 to Host H2, this step does not require any suspension of C1, and copies the bulk of the container file system to the destination server (H2). 
The next step involves checkpointing, and dumping the process in memory state to a file, this is what allows the container to be restarted from the same checkpointed state. 
However for sanity of the container process, it is important to restart the container from the same file system state as it was when the checkpoint was taken.
To ensure this, we take a second rsync of the ploop device of C1, and sync it with H2, after this the original container can be restarted.
Next we copy the dump file from step 3, from H1 to H2, and resume a new container from that dump file.

The overhead of cloning depends on the I/O operations happening within the container between step 2 and step 4 (the first and the second rsync), as this will increase the number of dirty pages in the memory, which in turn will impact the amount of memory that needs to be copied during the suspend phase (as mostly the dirty bits at suspend time are those which were not committed to memory and hence need to be transferred).  
A few iteration of cloning a container back and forth between two OpenVZ instances ( on KVM's within the same physical machine), resulted in an average suspend time of 1.8 seconds for the production container.
\fi
%( see table \ref{table:clonePerf}).
%add citation for OpenVZ Live Migration performance iteration testbed http://openvz.livejournal.com/47780.html
%This is nearly the same as that of native live migration\cite{openvzLiveMigrationPerf}, and has lesser suspend time for the production container as we do not include the \textit{``copy dump file''}, or the \textit{``undump and resume phase''} for production containers. 
%In section \ref{sec:performance}, we evaluate the performance of live cloning while doing increasing amount of random I/O write operations, as well as with doing page fetches  from web-server running the production container.

%The same amount of resources as the production container are reserved for the test-container. 
%After doing the pre-copy, we do a pause for syncing the two containers, and start them together. 
%Subsequent clone operations are optimized as they only require rsync for the change that has happened to the base image and in memory operation.
%After the intial setup, the frequency of cloning depends on the slowdown experienced by the test-container, and requirement of the test-case (some test cases may not require a long running test-window).

\iffalse

\begin{table}[t]
  \centering
    \begin{tabular}{ | p{2.2cm} | l | l | l | l | l | l | l |}
    \hline
    \textbf{Iteration} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{Avg} \\ \hline
    \textbf{Suspend + Dump} & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00\\ \hline
    \textbf{Pcopy after suspend} & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00\\ \hline
    \textbf{Copy Dump File} & 0.00 & 0.00 & 0.57 & 0.74 & 0.59 & 0.65\\ \hline
    \textbf{Undump and Resume} & 0.00 & 0.00 & 0.57 & 0.74 & 0.59 & 0.65\\ \hline
%    \textbf{--------------} & --- & --- & --- & --- & --- & --- & ---\\ \hline
    \textbf{Total Suspend Time} & 0.60 & 0.60 & 0.57 & 0.74 & 0.59 & 0.65\\ \hline
    \end{tabular}
\caption{Performance of Live Cloning (external mode) with a random file dump process running in the container}
\label{table:clonePerf}
\end{table}

\fi

\subsection{Customized Network Proxy} 
\label{sec:proxyDuplicator}

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.4\textwidth]{figs/network_dup.eps}
    \caption{Description of the Network Proxy. In \textit{synchronized} mode: there are 2 threads for each connection, Thread 1 executes steps [1,3,5], and Thread 2 executes [2,4,6] sequentially, hence the speed of packets sent to the production and the debug container are synchronized. In \textit{asynchronous} mode, there are 4 threads for each connection, Thread 1 executes steps [1,3], Thread 2 executes [2,4], Thread 3 executes [5], and Thread 4 executes [6]. Hence communication to the debug container and production container are asynchronous}
    \label{fig:duplicator}
  \end{center}
\end{figure}

In order for sandbox-testing to work, both containers must receive the same input.
The forwarding and duplication of packets to the production and debug-container, can be either synchronized (both send and receive responses together) or asynchronous.
In the synchronized mode the debug container, can impact the speed of the production container. 
The main reason for this is due to blocking sends being used to forward packets from the client to the debug-container and production container by the same process.
This would be undesirable, and we believe that asynchronous packet forwarding is a more acceptable solution for the proxy.

In the asynchronous packet forwarding mode (see figure \ref{fig:duplicator}), we use 4 threads T1, T2, T3, and T4 to manage each incoming connection to the proxy.
Thread T1 forwards packets from client to proxy (link 1), and from proxy to production container (link 2). 
It then uses a non-blocking send to forward packets to an \textbf{internal pipe buffer} shared between T1 and  T3. 
T3 then reads from this piped buffer and sends traffic forward to the debug-container. 
Similarly,  T2 receives packets from the production container and forwards them to the client, while T4 receives packets from the debug-container and drops them.
The \textbf{advantage of this asynchronous strategy is that any slowdown in the debug-container will not impact the production container's communication as a non-blocking send is used to forward traffic to the debug container}.
A similar proxy is designed for the traffic going forward from a mid-tier debug container. This proxy will cache the responses being sent to the production container, and relay them to the debug container. 

\textbf{Buffer Size } 
The pipe buffer in the proxy for the debug-container is generally over-provisioned to account for delayed execution in the debug-container. 
It's size is dependent on the amount of slow-down because of debugging, as well as the incoming workload.
As a safety precaution, we check for any network errors such as buffer overflows in the proxy to determine when the container is out-of-sync 
%A side-effect of this strategy is that if the speed of the debug-container is too slow compared to the production container, it may eventually lead to a buffer overflow. 
%The time taken by a connection before which it overflows is called it's \emph{testing-window}. 
The incoming request rate to the buffer is dependent on how fast the production container manages the requests (i.e. the production container is the rate-limiter).
Whereas the outgoing rate from the buffer is dependent on how fast the debug-container processes the requests, which can be higher than the production container because of added instrumentation, debugging etc.
Depending on the workload, the overhead induced in the debug-container, and the buffer size, the buffer may overflow.
The time period till buffer overflow happens is called the debugging-window.
For the duration of the debugging-window, the debug-container faithfully represents the state and executions in the production container. 
Once the buffer has overflown, the production container might need to be cloned again to ensure it has the same state.
It must be noted however, that the buffer size can be arbitrarily large, and network requests are generally very small (MySQL, HTTP requests are generally a few kilobytes), hence overflows will be rare.
 

